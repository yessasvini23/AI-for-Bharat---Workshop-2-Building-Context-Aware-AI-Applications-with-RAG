# -*- coding: utf-8 -*-
"""app.py (Streamlit Application).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZNhZ_C3vYlst1RYQhyD8-oABmqGoZIsZ
"""

import sys
!{sys.executable} -m pip install streamlit boto3
import streamlit as st
import boto3
import os
from dotenv import load_dotenv

# --- Configuration ---

# Load environment variables from .env file
load_dotenv()

# Set up Bedrock client
# Assumes AWS credentials/profile are configured locally
# The region is usually set by the profile or environment variable (AWS_REGION)
try:
    BEDROCK_AGENT_RUNTIME = boto3.client('bedrock-agent-runtime')
    KNOWLEDGE_BASE_ID = os.environ.get("KNOWLEDGE_BASE_ID", "YOUR_KB_ID_HERE") # Replace with your KB ID
    MODEL_ARN = os.environ.get("MODEL_ARN", "arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0")
except Exception as e:
    st.error(f"Error initializing Bedrock client. Ensure AWS credentials are set: {e}")
    st.stop()

# --- Streamlit UI Functions ---

def retrieve_and_generate_response(prompt: str):
    """
    Calls the Bedrock Knowledge Base API to retrieve context and generate a response.
    """
    if KNOWLEDGE_BASE_ID == "YOUR_KB_ID_HERE":
        st.warning("Please update KNOWLEDGE_BASE_ID in your environment variables or script.")
        return "Knowledge Base ID not configured.", []

    try:
        response = BEDROCK_AGENT_RUNTIME.retrieve_and_generate(
            input={'text': prompt},
            retrieveAndGenerateConfiguration={
                'type': 'KNOWLEDGE_BASE',
                'knowledgeBaseConfiguration': {
                    'knowledgeBaseId': KNOWLEDGE_BASE_ID,
                    'modelArn': MODEL_ARN,
                }
            }
        )

        # Extract the generated text and source citations
        generated_text = response['output']['text']
        citations = []

        # Process retrieval results for sources
        if 'retrievalResults' in response:
            for result in response['retrievalResults']:
                # The location object contains the S3 URI
                location_uri = result['location']['s3Location']['uri']
                citations.append(location_uri)

        return generated_text, citations

    except Exception as e:
        return f"An error occurred during RAG: {e}", []


# --- Streamlit App Setup ---

st.set_page_config(page_title="Context-Aware RAG Chatbot (AI for Bharat)", layout="wide")
st.title("ðŸ§  Context-Aware AI with RAG")
st.subheader("Query your proprietary documents using Amazon Bedrock Knowledge Base")

# Initialize chat history (to display past conversations)
if "messages" not in st.session_state:
    st.session_state.messages = []

# Display chat messages from history
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Handle user input
if prompt := st.chat_input("Ask a question about your documents..."):
    # 1. Add user message to chat history
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # 2. Get RAG response
    with st.spinner("Searching knowledge base and generating response..."):
        ai_response, sources = retrieve_and_generate_response(prompt)

    # 3. Add AI response to chat history
    with st.chat_message("assistant"):
        st.markdown(ai_response)

        # Display sources as an expander (critical for submission proof)
        if sources:
            with st.expander("ðŸ“„ **Sources/Citations Retrieved**"):
                # Use a set to display unique source URIs
                for i, source in enumerate(set(sources)):
                    st.markdown(f"**{i+1}.** {source}")
        else:
            st.info("No explicit sources were cited for this query.")

    st.session_state.messages.append({"role": "assistant", "content": ai_response})